# Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù†Ø¸Ø§Ù… Ø³ÙØ±ÙˆØ­ - Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ

Ø³Ø£Ù‚Ø¯Ù… Ù„Ùƒ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© ÙÙŠ Ù…Ù„ÙØ§Øª ÙˆØ§Ø­Ø¯Ø© Ù…ØªÙ…Ø§Ø³ÙƒØ© ÙˆØ¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªÙ†ØµÙŠØ¨ Ø§Ù„ÙÙˆØ±ÙŠ.

## Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

```
surooh_core/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ server.py
â”œâ”€â”€ brain.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_processor.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ integrations/
â”‚   â””â”€â”€ telegram_bot.py
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ memory.db
â”‚   â””â”€â”€ vector_store/
â”‚       â”œâ”€â”€ vector_index.faiss
â”‚       â””â”€â”€ vector_mapping.pkl
â””â”€â”€ tests/
    â”œâ”€â”€ test_brain.py
    â””â”€â”€ test_file_processor.py
```

## 1. Ù…Ù„Ù brain.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)

```python
import os
import sqlite3
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
import openai
import faiss
import numpy as np
import pickle
from datetime import datetime
import concurrent.futures

load_dotenv()

class SuroohBrain:
    def __init__(self):
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§ØªØµØ§Ù„ OpenAI
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        openai.api_key = self.openai_api_key
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.db_connection = sqlite3.connect('memory/memory.db', check_same_thread=False)
        self._init_db()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ
        self.vector_dim = 1536  # Ø£Ø¨Ø¹Ø§Ø¯ ØªØ¶Ù…ÙŠÙ† OpenAI
        self.vector_index = faiss.IndexFlatL2(self.vector_dim)
        self.vector_store_file = 'memory/vector_store/vector_index.faiss'
        self.vector_id_mapping = []
        self._load_vector_store()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.max_context_length = 4000  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)

    def _init_db(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        cursor = self.db_connection.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS conversations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            conversation_id TEXT NOT NULL UNIQUE,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            summary TEXT,
            category TEXT,
            tone TEXT,
            message_count INTEGER DEFAULT 0
        )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            conversation_id TEXT NOT NULL,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            embedding BLOB,
            FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
        )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS files (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_name TEXT NOT NULL,
            file_type TEXT NOT NULL,
            file_path TEXT NOT NULL,
            content TEXT,
            summary TEXT,
            category TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            conversation_id TEXT,
            FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
        )
        ''')
        
        self.db_connection.commit()

    def _load_vector_store(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§"""
        try:
            if os.path.exists(self.vector_store_file):
                self.vector_index = faiss.read_index(self.vector_store_file)
            
            mapping_file = 'memory/vector_store/vector_mapping.pkl'
            if os.path.exists(mapping_file):
                with open(mapping_file, 'rb') as f:
                    self.vector_id_mapping = pickle.load(f)
        except Exception as e:
            print(f"Warning: Could not load vector store - {str(e)}")
            # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø£
            self.vector_index = faiss.IndexFlatL2(self.vector_dim)
            self.vector_id_mapping = []

    def _save_vector_store(self):
        """Ø­ÙØ¸ Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª"""
        os.makedirs('memory/vector_store', exist_ok=True)
        faiss.write_index(self.vector_index, self.vector_store_file)
        
        with open('memory/vector_store/vector_mapping.pkl', 'wb') as f:
            pickle.dump(self.vector_id_mapping, f)

    def _get_embedding(self, text: str) -> List[float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ù…ØªØ¬Ù‡ÙŠ Ù„Ù„Ù†Øµ"""
        response = openai.Embedding.create(
            input=[text],
            model="text-embedding-ada-002"
        )
        return response['data'][0]['embedding']

    def _send_to_gpt(self, messages: List[Dict[str, str]], model: str = "gpt-4") -> Dict[str, Any]:
        """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¥Ù„Ù‰ GPT ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯"""
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=1500
            )
            return response.choices[0].message
        except Exception as e:
            raise Exception(f"Error in GPT communication: {str(e)}")

    def process_message(self, conversation_id: str, message: str, role: str = "user") -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ø¯"""
        # Ø­ÙØ¸ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ÙˆØ§Ø±Ø¯Ø©
        message_id = self._store_message(conversation_id, role, message)
        
        # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        context = self._get_conversation_context(conversation_id)
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ GPT (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        future = self.executor.submit(self._send_to_gpt, context)
        gpt_response = future.result()
        
        # Ø­ÙØ¸ Ø±Ø¯ GPT
        self._store_message(conversation_id, "assistant", gpt_response['content'])
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø¨Ø±Ø© ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø®Ù„ÙÙŠÙ‹Ø§ ÙˆØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        self.executor.submit(self._analyze_conversation, conversation_id)
        
        return {
            "response": gpt_response['content'],
            "conversation_id": conversation_id,
            "message_id": message_id,
            "context_length": len(context)
        }

    def _store_message(self, conversation_id: str, role: str, content: str) -> int:
        """ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        cursor = self.db_connection.cursor()
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        cursor.execute('''
        INSERT OR IGNORE INTO conversations (conversation_id) 
        VALUES (?)
        ''', (conversation_id,))
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ Ù„Ù„Ù†Øµ (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        embedding_future = self.executor.submit(self._get_embedding, content)
        
        # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        cursor.execute('''
        INSERT INTO messages (conversation_id, role, content)
        VALUES (?, ?, ?)
        ''', (conversation_id, role, content))
        
        message_id = cursor.lastrowid
        
        # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        cursor.execute('''
        UPDATE conversations 
        SET message_count = message_count + 1 
        WHERE conversation_id = ?
        ''', (conversation_id,))
        
        # Ø­ÙØ¸ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¹Ù†Ø¯ Ø§ÙƒØªÙ…Ø§Ù„Ù‡
        embedding = embedding_future.result()
        embedding_bytes = pickle.dumps(embedding)
        
        cursor.execute('''
        UPDATE messages 
        SET embedding = ? 
        WHERE id = ?
        ''', (embedding_bytes, message_id))
        
        self.db_connection.commit()
        
        # ØªØ­Ø¯ÙŠØ« ÙÙ‡Ø±Ø³ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        self.executor.submit(self._update_vector_index, message_id, content, embedding)
        
        return message_id

    def _update_vector_index(self, message_id: int, content: str, embedding: List[float]):
        """ØªØ­Ø¯ÙŠØ« ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© numpy
            vector = np.array([embedding], dtype='float32')
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³
            self.vector_index.add(vector)
            
            # Ø­ÙØ¸ Ø¹Ù„Ø§Ù‚Ø© ID
            self.vector_id_mapping.append(message_id)
            
            # Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø©
            self._save_vector_store()
        except Exception as e:
            print(f"Error updating vector index: {str(e)}")

    def _get_conversation_context(self, conversation_id: str, max_tokens: int = 3000) -> List[Dict[str, str]]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ²"""
        cursor = self.db_connection.cursor()
        
        cursor.execute('''
        SELECT role, content FROM messages 
        WHERE conversation_id = ?
        ORDER BY timestamp DESC
        LIMIT 20
        ''', (conversation_id,))
        
        messages = []
        total_tokens = 0
        
        for role, content in cursor.fetchall():
            # ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² (ØªÙ‚Ø±ÙŠØ¨ÙŠ)
            message_tokens = len(content.split()) * 1.33
            
            if total_tokens + message_tokens > max_tokens:
                break
                
            messages.insert(0, {"role": role, "content": content})
            total_tokens += message_tokens
        
        return messages

    def _analyze_conversation(self, conversation_id: str):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø© ÙˆØ§Ù„Ù†Ø¨Ø±Ø©"""
        try:
            context = self._get_conversation_context(conversation_id, max_tokens=2000)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø¨Ø±Ø©
            tone_prompt = {
                "role": "system",
                "content": "Analyze the tone of this conversation. Possible tones: angry, happy, neutral, frustrated, satisfied, curious. Respond with only one word."
            }
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¦Ø©
            category_prompt = {
                "role": "system",
                "content": "Categorize this conversation. Possible categories: sales, support, technical, billing, feedback, other. Respond with only one word."
            }
            
            # ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            summary_prompt = {
                "role": "system",
                "content": "Summarize this conversation in 2-3 sentences. Respond with only the summary."
            }
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
            with concurrent.futures.ThreadPoolExecutor() as executor:
                tone_future = executor.submit(self._send_to_gpt, [tone_prompt] + context)
                category_future = executor.submit(self._send_to_gpt, [category_prompt] + context)
                summary_future = executor.submit(self._send_to_gpt, [summary_prompt] + context)
                
                tone = tone_future.result()
                category = category_future.result()
                summary = summary_future.result()
            
            # ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            cursor = self.db_connection.cursor()
            cursor.execute('''
            UPDATE conversations 
            SET summary = ?, category = ?, tone = ? 
            WHERE conversation_id = ?
            ''', (summary['content'], category['content'], tone['content'], conversation_id))
            
            self.db_connection.commit()
        except Exception as e:
            print(f"Error analyzing conversation: {str(e)}")

    def search_memory(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_embedding = self._get_embedding(query)
            query_vector = np.array([query_embedding], dtype='float32')
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³
            distances, indices = self.vector_index.search(query_vector, k)
            
            # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©
            cursor = self.db_connection.cursor()
            results = []
            
            for idx in indices[0]:
                if idx < len(self.vector_id_mapping):
                    message_id = self.vector_id_mapping[idx]
                    cursor.execute('''
                    SELECT conversation_id, content, timestamp FROM messages WHERE id = ?
                    ''', (message_id,))
                    row = cursor.fetchone()
                    if row:
                        results.append({
                            "message_id": message_id,
                            "conversation_id": row[0],
                            "content": row[1],
                            "timestamp": row[2],
                            "distance": float(distances[0][idx])
                        })
            
            return sorted(results, key=lambda x: x['distance'])
        except Exception as e:
            print(f"Error searching memory: {str(e)}")
            return []

    def get_conversation_summary(self, conversation_id: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        cursor = self.db_connection.cursor()
        
        cursor.execute('''
        SELECT summary, category, tone, message_count FROM conversations
        WHERE conversation_id = ?
        ''', (conversation_id,))
        
        conv_data = cursor.fetchone()
        
        if not conv_data:
            return None
        
        cursor.execute('''
        SELECT MIN(timestamp), MAX(timestamp) FROM messages
        WHERE conversation_id = ?
        ''', (conversation_id,))
        
        time_data = cursor.fetchone()
        
        duration = 0
        if time_data and time_data[0] and time_data[1]:
            start = datetime.strptime(time_data[0], '%Y-%m-%d %H:%M:%S')
            end = datetime.strptime(time_data[1], '%Y-%m-%d %H:%M:%S')
            duration = (end - start).total_seconds()
        
        return {
            "summary": conv_data[0],
            "category": conv_data[1],
            "tone": conv_data[2],
            "message_count": conv_data[3],
            "duration_seconds": duration
        }

    def export_conversation(self, conversation_id: str, format: str = "json") -> Dict[str, Any]:
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        summary = self.get_conversation_summary(conversation_id)
        if not summary:
            return None
        
        cursor = self.db_connection.cursor()
        cursor.execute('''
        SELECT role, content, timestamp FROM messages
        WHERE conversation_id = ?
        ORDER BY timestamp ASC
        ''', (conversation_id,))
        
        messages = [
            {"role": row[0], "content": row[1], "timestamp": row[2]}
            for row in cursor.fetchall()
        ]
        
        result = {
            "conversation_id": conversation_id,
            "summary": summary,
            "messages": messages
        }
        
        if format == "csv":
            import csv
            from io import StringIO
            
            output = StringIO()
            writer = csv.writer(output)
            
            # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ø®Øµ
            writer.writerow(["Summary", summary['summary']])
            writer.writerow(["Category", summary['category']])
            writer.writerow(["Tone", summary['tone']])
            writer.writerow([])
            
            # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
            writer.writerow(["Role", "Content", "Timestamp"])
            for msg in messages:
                writer.writerow([msg['role'], msg['content'], msg['timestamp']])
            
            return {
                "content": output.getvalue(),
                "content_type": "text/csv"
            }
        else:
            return {
                "content": result,
                "content_type": "application/json"
            }
```

## 2. Ù…Ù„Ù server.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)

```python
from fastapi import FastAPI, UploadFile, File, HTTPException, Header, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import APIKeyHeader
from brain import SuroohBrain
from pydantic import BaseModel
from typing import Optional
import os
from datetime import datetime
import concurrent.futures

app = FastAPI(
    title="Surooh AI Core",
    version="1.0.0",
    description="The central brain of Surooh AI assistant"
)

# Ø¥Ø¹Ø¯Ø§Ø¯ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ
brain = SuroohBrain()

# Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
class Message(BaseModel):
    conversation_id: str
    message: str
    role: Optional[str] = "user"

class Conversation(BaseModel):
    conversation_id: str
    summary: Optional[str]
    category: Optional[str]
    tone: Optional[str]

class SearchQuery(BaseModel):
    query: str
    k: Optional[int] = 3

class ExportRequest(BaseModel):
    format: Optional[str] = "json"

# Ù…ØµØ§Ø¯Ù‚Ø© API
api_key_header = APIKeyHeader(name="X-API-Key")

def get_api_key(api_key: str = Depends(api_key_header)):
    if api_key != os.getenv("API_KEY"):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid API Key"
        )
    return api_key

# Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
@app.post("/message")
async def process_message(
    msg: Message, 
    api_key: str = Depends(get_api_key)
):
    try:
        response = brain.process_message(msg.conversation_id, msg.message, msg.role)
        return {"success": True, "data": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversation/{conversation_id}")
async def get_conversation(
    conversation_id: str,
    api_key: str = Depends(get_api_key)
):
    try:
        cursor = brain.db_connection.cursor()
        cursor.execute('''
        SELECT role, content, timestamp FROM messages 
        WHERE conversation_id = ?
        ORDER BY timestamp ASC
        ''', (conversation_id,))
        
        messages = [{"role": row[0], "content": row[1], "timestamp": row[2]} 
                  for row in cursor.fetchall()]
        
        summary = brain.get_conversation_summary(conversation_id)
        
        return {
            "success": True,
            "data": {
                "conversation_id": conversation_id,
                "messages": messages,
                "summary": summary
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/summary/{conversation_id}")
async def get_summary(
    conversation_id: str,
    api_key: str = Depends(get_api_key)
):
    try:
        summary = brain.get_conversation_summary(conversation_id)
        if not summary:
            raise HTTPException(status_code=404, detail="Conversation not found")
        
        return {"success": True, "data": summary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_memory(
    query: SearchQuery,
    api_key: str = Depends(get_api_key)
):
    try:
        results = brain.search_memory(query.query, query.k)
        return {"success": True, "data": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    conversation_id: Optional[str] = None,
    api_key: str = Depends(get_api_key)
):
    try:
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªÙ‹Ø§
        upload_dir = "memory/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        file_path = f"{upload_dir}/{file.filename}"
        
        with open(file_path, "wb") as buffer:
            buffer.write(file.file.read())
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        future = brain.executor.submit(brain.process_file, file_path, conversation_id)
        
        return {
            "success": True,
            "filename": file.filename,
            "message": "File is being processed",
            "processing_id": str(future)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/export/{conversation_id}")
async def export_conversation(
    conversation_id: str,
    format: str = "json",
    api_key: str = Depends(get_api_key)
):
    try:
        result = brain.export_conversation(conversation_id, format)
        if not result:
            raise HTTPException(status_code=404, detail="Conversation not found")
        
        if format == "csv":
            from fastapi.responses import PlainTextResponse
            return PlainTextResponse(
                content=result['content'],
                media_type=result['content_type'],
                headers={
                    "Content-Disposition": f"attachment; filename={conversation_id}.csv"
                }
            )
        else:
            return {"success": True, "data": result['content']}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "version": "1.0.0"}

if __name__ == "__main__":
    import uvicorn
    
    # Ø¨Ø¯Ø¡ ØªÙƒØ§Ù…Ù„ Telegram Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
    if os.getenv("TELEGRAM_BOT_TOKEN"):
        from integrations.telegram_bot import TelegramIntegration
        telegram_bot = TelegramIntegration(brain)
        telegram_bot.run()
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## 3. Ù…Ù„Ù utils/file_processor.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)

```python
import os
import pytesseract
from PIL import Image
import fitz  # PyMuPDF
from docx import Document
from typing import Dict, List
import concurrent.futures
import re

class FileProcessor:
    def __init__(self, brain):
        self.brain = brain
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
    
    def process_file(self, file_path: str, conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹Ù‡"""
        try:
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext in ['.jpg', '.jpeg', '.png']:
                result = self._process_image(file_path)
            elif file_ext == '.pdf':
                result = self._process_pdf(file_path)
            elif file_ext == '.docx':
                result = self._process_docx(file_path)
            elif file_ext == '.txt':
                result = self._process_text(file_path)
            else:
                raise ValueError(f"Unsupported file type: {file_ext}")
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self._store_file(file_path, result, conversation_id)
            
            return {
                "success": True,
                "file_path": file_path,
                "result": result
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "file_path": file_path
            }
    
    def _store_file(self, file_path: str, result: Dict[str, Any], conversation_id: Optional[str]):
        """ØªØ®Ø²ÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        cursor = self.brain.db_connection.cursor()
        
        file_name = os.path.basename(file_path)
        file_type = os.path.splitext(file_path)[1].lower().replace('.', '')
        
        cursor.execute('''
        INSERT INTO files (
            file_name, file_type, file_path, content, 
            summary, category, conversation_id
        ) VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            file_name, file_type, file_path, result.get('content'),
            result.get('summary'), result.get('category'), conversation_id
        ))
        
        self.brain.db_connection.commit()
    
    def _process_image(self, file_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR"""
        try:
            # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø©
            img = Image.open(file_path)
            width, height = img.size
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ØŒ Ù†Ù‚Ø³Ù…Ù‡Ø§ Ù„Ù‚Ø·Ø¹
            if width * height > 4000 * 4000:
                return self._process_large_image(img, file_path)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
            text = pytesseract.image_to_string(img)
            text = self._clean_text(text)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
            summary_future = self.executor.submit(self.brain._generate_summary, text)
            category_future = self.executor.submit(self.brain._categorize_content, text)
            
            summary = summary_future.result()
            category = category_future.result()
            
            return {
                "content": text,
                "summary": summary,
                "category": category,
                "type": "image"
            }
        except Exception as e:
            raise Exception(f"Image processing error: {str(e)}")
    
    def _process_large_image(self, img: Image.Image, file_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¨ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ù„Ù‚Ø·Ø¹"""
        text_parts = []
        summary_parts = []
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ù„Ù‚Ø·Ø¹ 2000x2000 Ø¨ÙƒØ³Ù„
        tile_size = 2000
        width, height = img.size
        
        for i in range(0, width, tile_size):
            for j in range(0, height, tile_size):
                box = (i, j, min(i + tile_size, width), min(j + tile_size, height))
                tile = img.crop(box)
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù‚Ø·Ø¹Ø©
                tile_text = pytesseract.image_to_string(tile)
                tile_text = self._clean_text(tile_text)
                
                if tile_text.strip():
                    text_parts.append(tile_text)
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ø®Øµ Ù„ÙƒÙ„ Ù‚Ø·Ø¹Ø© Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
                    future = self.executor.submit(self.brain._generate_summary, tile_text)
                    summary_parts.append(future)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        full_text = "\n".join(text_parts)
        
        # Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù„Ø®ØµØ§Øª
        summaries = [f.result() for f in summary_parts if f.result()]
        summary = "\n".join(summaries) if summaries else self.brain._generate_summary(full_text[:10000])
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø©
        sample_text = full_text[:5000] if len(full_text) > 5000 else full_text
        category = self.brain._categorize_content(sample_text)
        
        return {
            "content": full_text,
            "summary": summary,
            "category": category,
            "type": "image",
            "tiles_processed": len(text_parts)
        }
    
    def _process_pdf(self, file_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF Ø¨ØµÙØ­Ø§Øª"""
        try:
            text_parts = []
            summary_parts = []
            
            with fitz.open(file_path) as doc:
                for page_num, page in enumerate(doc):
                    page_text = page.get_text()
                    page_text = self._clean_text(page_text)
                    
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©
                    if not page_text.strip():
                        continue
                        
                    text_parts.append(f"\n--- Page {page_num+1} ---\n{page_text}")
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ ØµÙØ­Ø© Ø¹Ù„Ù‰ Ø­Ø¯Ø© Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
                    future = self.executor.submit(self.brain._generate_summary, page_text)
                    summary_parts.append((page_num+1, future))
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ
            full_text = "\n".join(text_parts)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„Ø®ØµØ§Øª
            summaries = [f"Page {num}: {f.result()}" for num, f in summary_parts if f.result()]
            summary = "\n".join(summaries) if summaries else self.brain._generate_summary(full_text[:10000])
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø©
            sample_text = full_text[:5000] if len(full_text) > 5000 else full_text
            category = self.brain._categorize_content(sample_text)
            
            return {
                "content": full_text,
                "summary": summary,
                "category": category,
                "type": "pdf",
                "page_count": len(text_parts)
            }
        except Exception as e:
            raise Exception(f"PDF processing error: {str(e)}")
    
    def _process_docx(self, file_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Word"""
        try:
            doc = Document(file_path)
            paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]
            full_text = "\n".join(paragraphs)
            full_text = self._clean_text(full_text)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ø®Øµ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
            summary_future = self.executor.submit(self.brain._generate_summary, full_text)
            category_future = self.executor.submit(self.brain._categorize_content, full_text)
            
            summary = summary_future.result()
            category = category_future.result()
            
            return {
                "content": full_text,
                "summary": summary,
                "category": category,
                "type": "docx"
            }
        except Exception as e:
            raise Exception(f"DOCX processing error: {str(e)}")
    
    def _process_text(self, file_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø³ÙŠØ·Ø©"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            
            full_text = self._clean_text(full_text)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ø®Øµ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
            summary_future = self.executor.submit(self.brain._generate_summary, full_text)
            category_future = self.executor.submit(self.brain._categorize_content, full_text)
            
            summary = summary_future.result()
            category = category_future.result()
            
            return {
                "content": full_text,
                "summary": summary,
                "category": category,
                "type": "text"
            }
        except Exception as e:
            raise Exception(f"Text file processing error: {str(e)}")
    
    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø·Ø¨Ø§Ø¹Ø©
        text = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', text)
        
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ù…Ø³Ø§ÙØ© ÙˆØ§Ø­Ø¯Ø©
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
```

## 4. Ù…Ù„Ù integrations/telegram_bot.py (Ø§Ù„Ø¬Ø¯ÙŠØ¯)

```python
import os
import threading
from telegram import Update, Bot
from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext
from brain import SuroohBrain

class TelegramIntegration:
    def __init__(self, brain: SuroohBrain):
        self.brain = brain
        self.bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
        if not self.bot_token:
            raise ValueError("TELEGRAM_BOT_TOKEN not found in environment variables")
        
        self.bot = Bot(token=self.bot_token)
        self.updater = Updater(token=self.bot_token, use_context=True)
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
        dp = self.updater.dispatcher
        dp.add_handler(CommandHandler("start", self.start))
        dp.add_handler(CommandHandler("summary", self.get_summary))
        dp.add_handler(MessageHandler(Filters.text & ~Filters.command, self.handle_message))
        
        # Ø¨Ø¯Ø¡ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª
        self.updater.start_polling()
    
    def start(self, update: Update, context: CallbackContext):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù…Ø± /start"""
        user = update.effective_user
        welcome_msg = f"""
Ù…Ø±Ø­Ø¨Ù‹Ø§ {user.first_name}! ğŸ‘‹
Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ø³ÙØ±ÙˆØ­ Ø§Ù„Ø°ÙƒÙŠ. ÙŠÙ…ÙƒÙ†Ù†ÙŠ:
- Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„ØªÙƒ Ø¨Ø°ÙƒØ§Ø¡
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„ØµÙˆØ±
- ØªØ°ÙƒØ± Ù…Ø­Ø§Ø¯Ø«Ø§ØªÙ†Ø§ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
- Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª

Ø§ÙƒØªØ¨ Ù„ÙŠ Ø£ÙŠ Ø´ÙŠØ¡ ÙˆØ³Ø£Ø³Ø§Ø¹Ø¯Ùƒ!
        """
        update.message.reply_text(welcome_msg)
    
    def handle_message(self, update: Update, context: CallbackContext):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù†ØµÙŠØ©"""
        user_id = update.message.from_user.id
        conversation_id = f"telegram_{user_id}"
        message = update.message.text
        
        try:
            # Ø¥Ø¸Ù‡Ø§Ø± Ø£Ù† Ø§Ù„Ø¨ÙˆØª ÙŠÙƒØªØ¨
            context.bot.send_chat_action(
                chat_id=update.effective_chat.id,
                action="typing"
            )
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø©
            response = self.brain.process_message(conversation_id, message)
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø¯
            update.message.reply_text(response['response'])
        except Exception as e:
            update.message.reply_text("âš ï¸ Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„ØªÙƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§.")
            print(f"Telegram bot error: {str(e)}")
    
    def get_summary(self, update: Update, context: CallbackContext):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù…Ø± /summary"""
        user_id = update.message.from_user.id
        conversation_id = f"telegram_{user_id}"
        
        try:
            summary = self.brain.get_conversation_summary(conversation_id)
            if not summary:
                update.message.reply_text("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ø³Ø¬Ù„Ø© Ø¨Ø¹Ø¯.")
                return
            
            summary_msg = f"""
ğŸ“ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
            
ğŸ”– Ø§Ù„ØªØµÙ†ÙŠÙ: {summary.get('category', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}
ğŸ­ Ø§Ù„Ù†Ø¨Ø±Ø©: {summary.get('tone', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©')}
ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„: {summary.get('message_count', 0)}
â³ Ø§Ù„Ù…Ø¯Ø©: {int(summary.get('duration_seconds', 0))} Ø«Ø§Ù†ÙŠØ©

ğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:
{summary.get('summary', 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ')}
            """
            update.message.reply_text(summary_msg)
        except Exception as e:
            update.message.reply_text("âš ï¸ Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ø®Øµ.")
            print(f"Telegram summary error: {str(e)}")
    
    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„"""
        self.updater.start_polling()
        print("Telegram bot is running...")
```

## 5. Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

`.env`:
```
OPENAI_API_KEY=your_openai_api_key_here
API_KEY=your_secret_api_key_here
TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here
```

`requirements.txt`:
```
fastapi==0.95.2
uvicorn==0.22.0
openai==0.27.8
python-dotenv==1.0.0
faiss-cpu==1.7.4
numpy==1.24.3
pytesseract==0.3.10
PyMuPDF==1.22.5
python-docx==0.8.11
Pillow==9.5.0
python-telegram-bot==20.3
pydantic==1.10.7
```

## ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØ¨

1. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
```bash
pip install -r requirements.txt
```

2. Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù„Ù `.env` Ø¨Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.

3. Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…:
```bash
python server.py
```

4. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ ÙˆØ§Ø¬Ù‡Ø© API Ø¹Ø¨Ø± `http://localhost:8000`

## Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

1. **Ù†Ø¸Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ù…ØªÙƒØ§Ù…Ù„** Ù…Ø¹ Ø±Ø¨Ø· ØµØ­ÙŠØ­ Ø¨ÙŠÙ† FAISS ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
2. **Ù…ØµØ§Ø¯Ù‚Ø© API** Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
3. **Ù…Ø¹Ø§Ù„Ø¬Ø© ÙØ¹Ø§Ù„Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©** (PDFØŒ Ø§Ù„ØµÙˆØ±) Ø¨ØµÙØ­Ø§Øª/Ù‚Ø·Ø¹
4. **Ù†Ù‚Ø§Ø· Ù†Ù‡Ø§ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©** Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„Ø®ØµØ§Øª ÙˆØ§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
5. **ØªÙƒØ§Ù…Ù„ Ø¬Ø§Ù‡Ø² Ù…Ø¹ Telegram** (Ø¹Ù†Ø¯ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙØªØ§Ø­)
6. **Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†Ø©** Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
7. **ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª** Ø¨ØµÙŠØºØªÙŠ JSON ÙˆCSV
8. **Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ** Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
9. **ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ** Ù„Ù„Ù†Ø¨Ø±Ø© ÙˆØ§Ù„ØªØµÙ†ÙŠÙ ÙˆØ§Ù„Ù…Ù„Ø®Øµ

Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ØªØ´Ù…Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆØªÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªÙ†ØµÙŠØ¨ Ø§Ù„ÙÙˆØ±ÙŠ ÙˆØ§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø¨ÙŠØ¦Ø© Ø¥Ù†ØªØ§Ø¬ÙŠØ©.